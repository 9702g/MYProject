import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import sklearn.datasets 
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor 
from sklearn import metrics
data=pd.read_csv(')
data['travel_with'] = data['travel_with'].replace(np.nan, 'Alone')
#replacing the nan values in the total_female column of the training data with 1.0

data['total_female'] = data['total_female'].replace(np.nan, 1.0)
#replacing the nan values in the total_male column of the training data with 1.0

data['total_male'] = data['total_male'].replace(np.nan, 1.0)

#replacing the nan values in the most_impressing column of the training data with No comments

data['most_impressing'] = data['most_impressing'].replace(np.nan, 'No comments')
#replacing 24-jan to 1-24 to correlate with 

data['age_group'] = data['age_group'].replace('24-Jan', '1-24')
# %convert float dtypes to int[total_female,total_male,night_mainland,night_zanzibar]
data["total_female"] = data['total_female'].astype('int')
data["total_male"] = data['total_male'].astype('int')
data["nights_spent"] = data['nights_spent'].astype('int')
# %Let's generate new features from some columns which makes some sense
data["total_people"] = data["total_female"] + data["total_male"]

data["total_nights"] = data["nights_spent"]
#chech for missing values
data.isnull().sum()
# %then it's time to encode objects into numeric

for colname in data.select_dtypes("object"):
    data[colname],_=data[colname].factorize()
# Now all columns that can be converted to numeric have been converted
# Step 3 model building
import warnings
warnings.filterwarnings('ignore')
x=data.drop(['total_cost'], axis=1)
y=data['total_cost']
#splitting the data into training data and test data
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size= 0.2,random_state= 2)
#model Training
#XGBOOST REGRESSOR
#loading the model
model=XGBRegressor()
#training the model with x_train
# Instantiate an object of XGBModel class
model = XGBRegressor()
# Call the fit method on the instantiated object
model.fit(X=x_train, y=y_train)
#Evaluation 
training_data_prediction=model.predict(x_train)
#R squared error 
score_1=metrics.r2_score(y_train,training_data_prediction)
#find the variants btwn both 
#mean absolute error 
score_2= metrics.mean_absolute_error(y_train,training_data_prediction)
#find difference and give mean
print('R squared error :', score_1)
print('Mean absolute Error:', score_2)
#prediction on training data
test_data_prediction=model.predict(x_test)
#R squared error 
score_1=metrics.r2_score(y_test,test_data_prediction)
#find the variants btwn both 
#mean absolute error 
score_2= metrics.mean_absolute_error(y_test,test_data_prediction)
#find difference and give mean
print('R squared error :', score_1)
print('Mean absolute Error:', score_2)
#visualizing the actual prices and predicted prices
#y train and y test
plt.scatter(y_train,training_data_prediction)
plt.xlabel("Actual prices")
plt.ylabel("predicted prices")
plt.title('Actual price vs predicted price')
plt.show
import matplotlib.pyplot as plt
plt.figure(figsize=(15,7))

plt.scatter(y_train,training_data_prediction,
          c = 'black', marker = 'o', s = 50, alpha = 0.5,
          label = 'Train data')
plt.scatter(y_test,test_data_prediction,
          c = 'g', marker = 'o', s = 50, alpha = 0.7,
          label = 'Test data')
plt.xlabel('Predicted values')
plt.ylabel('Tailings')
plt.legend(loc = 'upper right')
plt.show()


